{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랭체인(LangChain) 을 활용하여 허깅페이스(HuggingFace) 허브 \n",
    "출처 : 테디노트  \n",
    "랭체인은 허깅페이스 허브의 앤드포인트(Endpoint) 추론을 활용할 수 있는 래퍼(wrapper) 객체 및 함수를 제공하고 있습니다.   우리는 이를 활용하여 보다 쉽게 허깅페이스 모델을 활용한 서비스를 제작할 수 있습니다.  \n",
    "OpenAI 사의 ChatGPT 비용이 크다면, 공개된 허깅페이스 모델을 활용하는 것도 하나의 대안일 수 있습니다.  \n",
    "\n",
    "* HuggingFace Hub 소개\n",
    "\n",
    "Hugging Face Hub는 120k 이상의 모델, 20k의 데이터셋, 그리고 50k의 데모 앱(Spaces)을 포함하는 플랫폼입니다.  \n",
    "모든 것은 오픈 소스이며 공개적으로 이용할 수 있습니다.  \n",
    "이 플랫폼에서 사람들은 쉽게 협업하고 함께 ML을 구축할 수 있습니다.  \n",
    "아래 예시는 Hugging Face Hub에 연결하는 방법과 다양한 모델을 사용하는 방법을 보여줍니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain huggingface_hub transformers datasets dotenv langchain_community transformers \n",
    "pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "허깅페이스 토큰 발급  \n",
    "허깅페이스(https://huggingface.co) 에 회원가입을 한 뒤, 아래의 주소에서 토큰 발급을 신청합니다.  \n",
    "\n",
    "토큰 발급주소: https://huggingface.co/docs/hub/security-tokens  \n",
    "토큰을 발급받은 뒤 아래 주소에서 LLM 의 READ 키를 복사합니다.  \n",
    "\n",
    "LLM 키: https://huggingface.co/settings/tokens  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv('../.env')  # .env 파일의 환경변수 로드\n",
    "os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! 추론에 활용할 모델 선택  \n",
    "허깅페이스 LLM 리더보드: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard  \n",
    "\n",
    "리더보드에 게재된 모델의 성능을 직접 확인할 수 있으며, 모델의 ID 만 알고 있으면 됩니다.  \n",
    "예를 들어, 위의 그림 기준으로 첫 번째 랭크되어 있는 모델의 ID 는 AIDC-ai-business/Marcoroni-70B-v1 입니다.  \n",
    "한글 LLM 리더보드: https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스 0.19.4 + langchain 0.3.25 \n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "## beomi/KoAlpaca-Polyglot-5.8B모델 사용하기 (메모리기반의 한국어모델)\n",
    "\"\"\"\n",
    "beomi/KoAlpaca-Polyglot-5.8B는 Hugging Face Inference API로 사용할 수 없습니다.\n",
    "반드시 로컬에서 transformers로만 사용해야 합니다.\n",
    "Inference API로 사용하려면, \"Use via API\"가 있는 모델을 선택하세요.\n",
    "\"\"\"\n",
    "\n",
    "repo_id = \"beomi/KoAlpaca-Polyglot-5.8B\"\n",
    "question = \"대한민국 수도는 어디인가요?\"\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# HuggingFaceHub 객체 생성\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id, \n",
    "    model_kwargs={\"temperature\": 0.2, \n",
    "                  \"max_length\": 128}\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "print(llm_chain.run(question=question)) #### 문제점 추적필요 \n",
    "\n",
    "### 해당 모델의 경우 API 사용 불가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "## torch 설치 필요 ( 단 python 3.13 에서 torch 설치 불가)\n",
    "\n",
    "# text-generation 파이프라인 생성\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"beomi/KoAlpaca-Polyglot-5.8B\"\n",
    ")\n",
    "\n",
    "# 프롬프트 입력\n",
    "prompt = \"대한민국의 수도는 어디인가요?\"\n",
    "\n",
    "# 텍스트 생성\n",
    "result = pipe(prompt, max_new_tokens=128, temperature=0.2)\n",
    "\n",
    "# 결과 출력\n",
    "print(result[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
