{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랭체인(langchain) + 챗(chat) - ConversationChain, 템플릿 사용법\n",
    "출처 : 테디노트 \n",
    "\n",
    "- LangChain + Chat 구성  \n",
    "채팅 모델(Chat Model): LLM을 활용한 대화 흐름(conversation) 을 생성할 수 있습니다.  \n",
    "프롬프트 템플릿(Prompt Template): 프롬프트 템플릿은 기본 메시지, 사용자 입력, 채팅 기록, 문맥(context) 를 결합하여 프롬프트를 쉽게 구성할 수 있게 돕습니다.  \n",
    "메모리(Memory): 대화 흐름에서 대화의 내용을 기록하는 용도입니다.  \n",
    "리트리버(Retriever): 문서의 로드 및 검색을 할 때 활용할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain openai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## open ai token \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv('../.env')  # .env 파일의 환경변수 로드\n",
    "os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3d/dxbhq4214vqddsxgwxm891gm0000gn/T/ipykernel_8813/2389360662.py:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  chat = ChatOpenAI()\n",
      "/var/folders/3d/dxbhq4214vqddsxgwxm891gm0000gn/T/ipykernel_8813/2389360662.py:8: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chat(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I really love programming.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 37, 'total_tokens': 42, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--8723623f-ac2d-4280-a89c-d53e0890c30f-0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# LLM 생성\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "# 실행\n",
    "chat(\n",
    "    [HumanMessage(content=\"다음을 영어로 번역해 줘: 나는 프로그래밍을 정말 좋아해요\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 메시지 구성요소\n",
    "\n",
    "- SystemMessage: AI 에게 상황을 지정하거나 역할 부여\n",
    "- HumanMessage: 사용자 입력 메시기\n",
    "- AIMessage: AI 답변 메시지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I really love programming.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 51, 'total_tokens': 56, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--65969458-14be-49d1-9b77-f6441891d477-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    # SystemMessage: 역할 부여\n",
    "    SystemMessage(content=\"너는 한글을 영어로 번역해 주는 전문번역가야.\"),\n",
    "    # 질의\n",
    "    HumanMessage(content=\"나는 프로그래밍을 정말 좋아해요\")\n",
    "]\n",
    "\n",
    "# 실행\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='프로그래밍을 좋아한다는 것은 어떤 측면에서 그렇게 느끼는지 알려주실 수 있나요? 프로그래밍을 통해 어떤 즐거움을 느끼는지, 어떤 종류의 프로그래밍을 선호하는지 등을 설명해주시면 좀 더 자세한 분석을 도와드릴 수 있을 것 같아요.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 132, 'prompt_tokens': 49, 'total_tokens': 181, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--136f8b2d-1b82-41bf-b1d0-64e31348b479-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    # SystemMessage: 역할 부여\n",
    "    SystemMessage(content=\"너는 심리를 분석하는 심리 분석가야.\"),\n",
    "    # 질의\n",
    "    HumanMessage(content=\"나는 프로그래밍을 정말 좋아해요\")\n",
    "]\n",
    "\n",
    "# 실행\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='프로그래밍을 좋아하는 사람들은 주로 INTJ나 INTP 유형에 속할 가능성이 높습니다. INTJ는 전략적이고 분석적인 사고를 가지고 있으며, 문제 해결 능력이 뛰어나고 새로운 아이디어를 추구합니다. 한편, INTP는 논리적이고 타당성 있는 이유를 중요시하며, 새로운 아이디어에 호기심을 가지고 흥미를 느낍니다. 그렇기 때문에 이 두 유형 중 하나에 속할 가능성이 크다고 볼 수 있습니다. 하지만 MBTI 유형은 개인의 성향과 특징을 나타내는 것이므로, 정확한 유형을 알기 위해서는 MBTI 검사를 통해 확인하는 것이 좋습니다.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 256, 'prompt_tokens': 39, 'total_tokens': 295, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--e918c5b3-049d-44cb-8fe8-4020421a14c9-0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    # SystemMessage: 역할 부여\n",
    "    SystemMessage(content=\"너는 MBTI 예측가야.\"),\n",
    "    # 질의\n",
    "    HumanMessage(content=\"나는 프로그래밍을 정말 좋아해요\")\n",
    "]\n",
    "\n",
    "# 실행\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationChain  \n",
    "이전의 채팅 모델을 ConversationChain 으로 감쌀 수 있습니다.  \n",
    "ConversationChain 는 한마디로 (Chat Model + Memory) 이라 정의할 수 있습니다.  \n",
    "  \n",
    "이전의 Chat Model 은 대화내용을 저장 혹은 기록하는 작업은 직접해야 합니다. 하지만 ConversationChain 을 활용하면, 자동으로 Memory 에 대화내용을 저장합니다.  \n",
    "즉, ConversationChain 자체가 내장 메모리 를 가지고 있습니다. 따라서, 이전의 대화내용을 기록하는 메모리에 대한 별도 작업을 생략할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3d/dxbhq4214vqddsxgwxm891gm0000gn/T/ipykernel_8813/4041378486.py:4: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(llm=chat)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pydantic/main.py:253: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "/var/folders/3d/dxbhq4214vqddsxgwxm891gm0000gn/T/ipykernel_8813/4041378486.py:6: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(conversation.run(\"다음을 영어로 번역해 줘: 나는 프로그래밍을 정말 좋아해요\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! \"나는 프로그래밍을 정말 좋아해요\" translates to \"I really like programming\" in English. Do you have any other translations you need help with?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain  \n",
    "\n",
    "# ConversationChain 객체 생성\n",
    "conversation = ConversationChain(llm=chat)\n",
    "# 1차 질의\n",
    "print(conversation.run(\"다음을 영어로 번역해 줘: 나는 프로그래밍을 정말 좋아해요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! \"나는 운동도 정말 좋아해요\" translates to \"I also really like exercising\" in English. Is there anything else you'd like to translate?\n"
     ]
    }
   ],
   "source": [
    "## 2차 질의를 하면, 이전 대화내용을 바탕으로 답변을 얻습니다.\n",
    "print(conversation.run(\"나는 운동도 정말 좋아해요\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ① 템플릿으로 양식 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3d/dxbhq4214vqddsxgwxm891gm0000gn/T/ipykernel_8813/478569817.py:36: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  conversation = LLMChain(\n",
      "/var/folders/3d/dxbhq4214vqddsxgwxm891gm0000gn/T/ipykernel_8813/478569817.py:44: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = conversation({\"human_input\": \"나는 프로그래밍을 정말 좋아해요\"})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 너는 한글을 영어로 번역해 주는 번역전문가야.\n",
      "Human: 나는 프로그래밍을 정말 좋아해요\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I really love programming.\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "# 1) 시스템 설정: 역할부여 정의\n",
    "system_template = SystemMessagePromptTemplate.from_template(\"너는 한글을 {language}로 번역해 주는 번역전문가야.\")\n",
    "system_message = system_template.format(language=\"영어\")\n",
    "\n",
    "# 2) ChatPromptTemplate 템플릿 정의\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,                                              # 역할부여\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),           # 메모리 저장소 설정. ConversationBufferMemory의 memory_key 와 동일하게 설정\n",
    "    HumanMessagePromptTemplate.from_template(\"{human_input}\"),   # 사용자 메시지 injection\n",
    "])\n",
    "\n",
    "\n",
    "# 3) 메모리 정의\n",
    "# `return_messages=True`를 사용해야 MessagesPlaceholder에 주입됨\n",
    "# `chat_history`가 MessagesPlaceholder 이름과 일치해야함\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", \n",
    "                                  ai_prefix=\"번역가\",\n",
    "                                  human_prefix=\"사용자\",\n",
    "                                  return_messages=True)\n",
    "\n",
    "\n",
    "# 4) LLM 모델 정의\n",
    "llm = ChatOpenAI(model_name='gpt-4.1-nano')\n",
    "\n",
    "# 5) LLMChain 정의\n",
    "conversation = LLMChain(\n",
    "    llm=llm,       # LLM\n",
    "    prompt=prompt, # Prompt\n",
    "    verbose=True,  # True 로 설정시 로그 출력\n",
    "    memory=memory  # 메모리\n",
    ")\n",
    "\n",
    "# 6) 실행\n",
    "answer = conversation({\"human_input\": \"나는 프로그래밍을 정말 좋아해요\"})\n",
    "print(answer['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 너는 한글을 영어로 번역해 주는 번역전문가야.\n",
      "Human: 나는 프로그래밍을 정말 좋아해요\n",
      "AI: I really love programming.\n",
      "Human: 손흥민은 대한민국의 축구선수 입니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Son Heung-min is a South Korean football player.\n"
     ]
    }
   ],
   "source": [
    "answer = conversation({\"human_input\": \"손흥민은 대한민국의 축구선수 입니다.\"})\n",
    "print(answer['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ② 대화내용 초기화\n",
    "- 대화 문맥(conversation context)을 초기화 하는 방법은 간단합니다. memory 를 clear() 함수를 실행하여 초기화 해주면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': []}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 대화목록 출력\n",
    "memory.load_memory_variables({})\n",
    "# 대화내용 초기화\n",
    "memory.clear()\n",
    "# 대화목록 출력\n",
    "memory.load_memory_variables({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 너는 한글을 영어로 번역해 주는 번역전문가야.\n",
      "Human: 손흥민은 대한민국의 축구선수 입니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Son Heung-min is a South Korean soccer player.\n"
     ]
    }
   ],
   "source": [
    "## SystemMessage 는 메모리 초기화 후에도 여전히 적용됩니다.\n",
    "\n",
    "# 실행\n",
    "answer = conversation({\"human_input\": \"손흥민은 대한민국의 축구선수 입니다.\"})\n",
    "print(answer['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ③ 대화내용 요약 메모리\n",
    "ConversationSummaryBufferMemory 는최근의 대화 내용을 메모리에 저장합니다.  \n",
    "  \n",
    "다만, 대화가 길어질 경우 이전 대화내용을 모두 포함할 수 없습니다. 따라서, 이전 대화 내용을 삭제하게 되는데,  ConversationSummaryBufferMemory 는 완전히 삭제하기보다는 요약 정보로 만듭니다.  \n",
    "  \n",
    "이전 대화내용에 대한 요약이 발동하는 시점은 대화의 interaction 횟수가 초과된 경우 혹은 토큰 길이가 초과된 경우를 기준으로 동작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 너는 회사의 마케팅 담당자야. 다음의 상품 정보를 바탕으로 고객 응대를 해줘.\n",
      "\n",
      "상품명: 하늘을 나는 스마트폰\n",
      "제품가격: 300만원\n",
      "주요기능:\n",
      "- 스마트폰을 공중에 띄울 수 있음\n",
      "- 방수기능\n",
      "- 최신식 인공지능 솔루션 탑재\n",
      "- 전화통화\n",
      "- 음악\n",
      "- 인터넷 브라우징, 5G 네트워크, 와이파이, 블루투스\n",
      "제조사: 파인애플\n",
      "\n",
      "Human: 저에게 추천해 줄만한 상품이 있나요?\u001b[0m\n",
      "안녕하세요! 고객님께 추천드리고 싶은 제품은 바로 '하늘을 나는 스마트폰'입니다. 이 제품은 300만원으로, 다음과 같은 뛰어난 기능들이 탑재되어 있어 일상생활을 더욱 편리하고 즐겁게 만들어 드립니다.\n",
      "\n",
      "- 스마트폰을 공중에 띄울 수 있어 독특한 경험을 선사합니다.\n",
      "- 방수 기능이 탑재되어 물이나 습기에 강하며 안전하게 사용할 수 있습니다.\n",
      "- 최신 인공지능 솔루션이 탑재되어 개인 맞춤형 서비스와 효율적인 사용이 가능합니다.\n",
      "- 전화통화, 음악 감상, 인터넷 브라우징(5G 네트워크, 와이파이, 블루투스 지원) 등 다양한 기능을 한 번에 즐기실 수 있습니다.\n",
      "\n",
      "파인애플이 제조한 이 혁신적인 제품으로 일상에 새로운 차원을 더해보시는 건 어떠실까요? 더 궁금하신 점이나 구매 상담이 필요하시면 언제든 문의 주세요!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model cl100k_base.See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     45\u001b[39m conversation = LLMChain(\n\u001b[32m     46\u001b[39m     llm=llm,       \u001b[38;5;66;03m# LLM\u001b[39;00m\n\u001b[32m     47\u001b[39m     prompt=prompt, \u001b[38;5;66;03m# Prompt\u001b[39;00m\n\u001b[32m     48\u001b[39m     verbose=\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# True 로 설정시 로그 출력\u001b[39;00m\n\u001b[32m     49\u001b[39m     memory=memory  \u001b[38;5;66;03m# 메모리\u001b[39;00m\n\u001b[32m     50\u001b[39m )\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# 6) 실행\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m answer = \u001b[43mconversation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuman_input\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m저에게 추천해 줄만한 상품이 있나요?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03m!! 첫번째 에러 : ImportError: Could not import tiktoken python package. This is needed in order to calculate get_token_ids. Please install it with `pip install tiktoken`.\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[33;03m실행은 되지만 tiktoken 에러 발생 pip install tiktoken 실행해 줘야 함\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m \n\u001b[32m     61\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/_api/deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py:386\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    355\u001b[39m \n\u001b[32m    356\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    377\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    379\u001b[39m config = {\n\u001b[32m    380\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    381\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    382\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    384\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py:162\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m    157\u001b[39m         \u001b[38;5;28mself\u001b[39m._call(inputs, run_manager=run_manager)\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprep_outputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/chains/base.py:463\u001b[39m, in \u001b[36mChain.prep_outputs\u001b[39m\u001b[34m(self, inputs, outputs, return_only_outputs)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_outputs(outputs)\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_only_outputs:\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/memory/summary_buffer.py:96\u001b[39m, in \u001b[36mConversationSummaryBufferMemory.save_context\u001b[39m\u001b[34m(self, inputs, outputs)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;28msuper\u001b[39m().save_context(inputs, outputs)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprune\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain/memory/summary_buffer.py:108\u001b[39m, in \u001b[36mConversationSummaryBufferMemory.prune\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Prune buffer if it exceeds max token limit\"\"\"\u001b[39;00m\n\u001b[32m    107\u001b[39m buffer = \u001b[38;5;28mself\u001b[39m.chat_memory.messages\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m curr_buffer_length = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_num_tokens_from_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m curr_buffer_length > \u001b[38;5;28mself\u001b[39m.max_token_limit:\n\u001b[32m    110\u001b[39m     pruned_memory = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_community/chat_models/openai.py:675\u001b[39m, in \u001b[36mChatOpenAI.get_num_tokens_from_messages\u001b[39m\u001b[34m(self, messages, tools)\u001b[39m\n\u001b[32m    673\u001b[39m     tokens_per_name = \u001b[32m1\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    676\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mget_num_tokens_from_messages() is not presently implemented \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    677\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfor model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    678\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/openai/openai-python/blob/main/chatml.md for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    679\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minformation on how messages are converted to tokens.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    680\u001b[39m     )\n\u001b[32m    681\u001b[39m num_tokens = \u001b[32m0\u001b[39m\n\u001b[32m    682\u001b[39m messages_dict = [convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "\u001b[31mNotImplementedError\u001b[39m: get_num_tokens_from_messages() is not presently implemented for model cl100k_base.See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens."
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "# 1) 시스템 설정: 역할부여 정의\n",
    "system_template = SystemMessagePromptTemplate.from_template(\n",
    "    \"너는 회사의 마케팅 담당자야. 다음의 상품 정보를 바탕으로 고객 응대를 해줘.\\n{product}\"\n",
    ")\n",
    "system_message = system_template.format(product=\"\"\"\n",
    "상품명: 하늘을 나는 스마트폰\n",
    "제품가격: 300만원\n",
    "주요기능:\n",
    "- 스마트폰을 공중에 띄울 수 있음\n",
    "- 방수기능\n",
    "- 최신식 인공지능 솔루션 탑재\n",
    "- 전화통화\n",
    "- 음악\n",
    "- 인터넷 브라우징, 5G 네트워크, 와이파이, 블루투스\n",
    "제조사: 파인애플\n",
    "\"\"\")\n",
    "\n",
    "# 2) ChatPromptTemplate 템플릿 정의\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,                                              # 역할부여\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),           # 메모리 저장소 설정. ConversationBufferMemory의 memory_key 와 동일하게 설정\n",
    "    HumanMessagePromptTemplate.from_template(\"{human_input}\"),   # 사용자 메시지 injection\n",
    "])\n",
    "\n",
    "# 4) LLM 모델 정의\n",
    "llm = ChatOpenAI(model_name='gpt-4.1-nano', \n",
    "                 temperature=0.5,\n",
    "                 streaming=True,\n",
    "                 callbacks=[StreamingStdOutCallbackHandler()])\n",
    "\n",
    "\n",
    "# 3) 메모리 정의\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, \n",
    "                                         memory_key=\"chat_history\", \n",
    "                                         max_token_limit=10, \n",
    "                                         return_messages=True\n",
    "                                        )\n",
    "\n",
    "\n",
    "# 5) LLMChain 정의\n",
    "conversation = LLMChain(\n",
    "    llm=llm,       # LLM\n",
    "    prompt=prompt, # Prompt\n",
    "    verbose=True,  # True 로 설정시 로그 출력\n",
    "    memory=memory  # 메모리\n",
    ")\n",
    "\n",
    "# 6) 실행\n",
    "answer = conversation({\"human_input\": \"저에게 추천해 줄만한 상품이 있나요?\"})\n",
    "\n",
    "\"\"\"\n",
    "!! 첫번째 에러 : ImportError: Could not import tiktoken python package. This is needed in order to calculate get_token_ids. Please install it with `pip install tiktoken`.\n",
    "실행은 되지만 tiktoken 에러 발생 pip install tiktoken 실행해 줘야 함\n",
    "\n",
    "!! 두번째 에러 : NotImplementedError: get_num_tokens_from_messages() is not presently implemented for model cl100k_base. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\n",
    "model_name 을 명시적으로 지정해줘야 함 명시해 줬는데도 에러가 남 (미해결)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
